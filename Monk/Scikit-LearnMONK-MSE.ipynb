{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Title: The Monk's Problems (CLASSIFICATION PROBLEMS)\n",
    "\n",
    "4. Relevant Information:\n",
    "\n",
    "   The MONK's problem were the basis of a first international comparison\n",
    "   of learning algorithms. The result of this comparison is summarized in\n",
    "   \"The MONK's Problems - A Performance Comparison of Different Learning\n",
    "   algorithms\" by S.B. Thrun, J. Bala, E. Bloedorn, I.  Bratko, B.\n",
    "   Cestnik, J. Cheng, K. De Jong, S.  Dzeroski, S.E. Fahlman, D. Fisher,\n",
    "   R. Hamann, K. Kaufman, S. Keller, I. Kononenko, J.  Kreuziger, R.S.\n",
    "   Michalski, T. Mitchell, P.  Pachowicz, Y. Reich H.  Vafaie, W. Van de\n",
    "   Welde, W. Wenzel, J. Wnek, and J. Zhang has been published as\n",
    "   Technical Report CS-CMU-91-197, Carnegie Mellon University in Dec.\n",
    "   1991.\n",
    "\n",
    "   One significant characteristic of this comparison is that it was\n",
    "   performed by a collection of researchers, each of whom was an advocate\n",
    "   of the technique they tested (often they were the creators of the\n",
    "   various methods). In this sense, the results are less biased than in\n",
    "   comparisons performed by a single person advocating a specific\n",
    "   learning method, and more accurately reflect the generalization\n",
    "   behavior of the learning techniques as applied by knowledgeable users.\n",
    "\n",
    "   There are three MONK's problems.  The domains for all MONK's problems\n",
    "   are the same (described below).  One of the MONK's problems has noise\n",
    "   added. For each problem, the domain has been partitioned into a train\n",
    "   and test set.\n",
    "\n",
    "5. Number of Instances: 432\n",
    "\n",
    "6. Number of Attributes: 8 (including class attribute)\n",
    "\n",
    "7. Attribute information:\n",
    "    1. class: 0, 1 \n",
    "    2. a1:    1, 2, 3\n",
    "    3. a2:    1, 2, 3\n",
    "    4. a3:    1, 2\n",
    "    5. a4:    1, 2, 3\n",
    "    6. a5:    1, 2, 3, 4\n",
    "    7. a6:    1, 2\n",
    "    8. Id:    (A unique symbol for each instance)\n",
    "\n",
    "8. Missing Attribute Values: None\n",
    "\n",
    "9. Target Concepts associated to the MONK's problem:\n",
    "\n",
    "   MONK-1: (a1 = a2) or (a5 = 1)\n",
    "\n",
    "   MONK-2: EXACTLY TWO of {a1 = 1, a2 = 1, a3 = 1, a4 = 1, a5 = 1, a6 = 1}\n",
    "\n",
    "   MONK-3: (a5 = 3 and a4 = 1) or (a5 /= 4 and a2 /= 3)\n",
    "           (5% class noise added to the training set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# librerie di base dal file utils.py\n",
    "from utils import *\n",
    "\n",
    "# librerie utili per la classificazione\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# per rimuovere i warnings\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monk 1\n",
    "MONK-1: (a1 = a2) or (a5 = 1) --> Class label 1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# lettura dataset monk 1\n",
    "monk_train_1 = load_monk_data_one_hot_enc(\"../data/monk+s+problems/monks-1.train\")\n",
    "monk_test_1 = load_monk_data_one_hot_enc(\"../data/monk+s+problems/monks-1.test\")\n",
    "\n",
    "monk_train_1.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(monk_train_1.shape)\n",
    "print(monk_test_1.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# separazione feature e target\n",
    "X_dev, y_dev = monk_train_1.iloc[:, 1:], monk_train_1.iloc[:, 0]\n",
    "X_test, y_test = monk_test_1.iloc[:, 1:], monk_test_1.iloc[:, 0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(X_dev.shape)\n",
    "print(y_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Splitting del dev set in training e validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "batch_size = X_train.shape[0]\n",
    "print(batch_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Iperparametri da testare \n",
    "iperparameters = [ \n",
    "    {\n",
    "    'hidden_layer_sizes': [(2,), (3,), (4,)],  \n",
    "    'activation': ['tanh', 'relu', 'logistic'],  \n",
    "    'solver': ['adam'], \n",
    "    'learning_rate_init': [0.01, 0.02, 0.05, 0.001, 0.002],\n",
    "    },\n",
    "    {\n",
    "    'hidden_layer_sizes': [(2,), (3,), (4,)],  \n",
    "    'activation': ['tanh', 'relu', 'logistic'],  \n",
    "    'solver': ['sgd'],  \n",
    "    'learning_rate_init': [0.01, 0.02, 0.05, 0.001, 0.002],\n",
    "    'learning_rate': ['constant', 'adaptive'],  # solo per SGD\n",
    "    'momentum': [0.9, 0.95]  # solo per SGD\n",
    "    }\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_train_acc = float('-inf')\n",
    "best_val_acc = float('-inf')\n",
    "best_train_mse = float('inf')\n",
    "best_val_mse = float('inf')\n",
    "best_params = None\n",
    "\n",
    "# Itera su tutte le combinazioni di iperparametri\n",
    "for params in ParameterGrid(iperparameters):\n",
    "    # Definizione del modello con gli iperparametri scelti\n",
    "    nn = MLPRegressor(\n",
    "        hidden_layer_sizes=params['hidden_layer_sizes'],\n",
    "        activation=params['activation'],\n",
    "        solver=params['solver'],\n",
    "        learning_rate_init=params['learning_rate_init'],\n",
    "        batch_size=batch_size,  # fullbatch (99)\n",
    "        **({'learning_rate': params['learning_rate']} if params['solver'] == 'sgd' else {}),  # learning_rate solo se il solver è 'sgd'\n",
    "        **({'momentum': params['momentum']} if params['solver'] == 'sgd' else {}),  # momentum solo se il solver è 'sgd'\n",
    "        max_iter=500,\n",
    "        shuffle=True,\n",
    "        random_state=7,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Addestramento del modello \n",
    "    nn.fit(X_train, y_train)\n",
    "\n",
    "    # Predizioni sui dati\n",
    "    y_pred_train = nn.predict(X_train)\n",
    "    y_pred_val = nn.predict(X_val)\n",
    "\n",
    "    # Calcolo MSE\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    val_mse = mean_squared_error(y_val, y_pred_val)\n",
    "\n",
    "    # Convertsione delle predizioni in classi discrete\n",
    "    train_acc = accuracy_score(y_train.round(), y_pred_train.round())\n",
    "    val_acc = accuracy_score(y_val.round(), y_pred_val.round())\n",
    "\n",
    "\n",
    "    # Verifica se abbiamo trovato un modello migliore\n",
    "    if val_acc > best_val_acc:  \n",
    "        best_train_mse = train_mse\n",
    "        best_val_mse = val_mse\n",
    "        best_train_acc = train_acc\n",
    "        best_val_acc = val_acc\n",
    "        best_model = nn\n",
    "        best_params = params"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "print(\"\\nMiglior modello trovato per MONK-1 (random state 7 - 500 epochs):\")\n",
    "print(f\"Parametri: {best_params}\")\n",
    "\n",
    "print(F'Training Accuracy: {accuracy_score(y_train, best_model.predict(X_train).round()):.4f}')\n",
    "print(f'Validation Accuracy: {accuracy_score(y_val, best_model.predict(X_val).round()):.4f}')\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, best_model.predict(X_test).round()):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Inizializzazione modello con i miglior iperparametri trovati\n",
    "nn = MLPRegressor(\n",
    "        hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "        activation=best_params['activation'],\n",
    "        solver=best_params['solver'],\n",
    "        learning_rate_init=best_params['learning_rate_init'],\n",
    "        batch_size=batch_size,  # fullbatch (99)\n",
    "        **({'learning_rate': best_params['learning_rate']} if best_params['solver'] == 'sgd' else {}), # learning_rate solo se il solver è 'sgd'\n",
    "        **({'momentum': best_params['momentum']} if best_params['solver'] == 'sgd' else {}), # momentum solo se il solver è 'sgd'\n",
    "        max_iter=1, # nota bene\n",
    "        warm_start=True, # nota bene\n",
    "        shuffle=True,\n",
    "        random_state=7,\n",
    "        verbose=False\n",
    "      )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TRAINING MODELLO\n",
    "\n",
    "# Numero di epoche\n",
    "epochs = 500  \n",
    "\n",
    "# Liste per tenere traccia delle metriche epoca per epoca\n",
    "train_loss_curve = []\n",
    "test_loss_curve = []\n",
    "train_accuracy_curve = []\n",
    "test_accuracy_curve = []\n",
    "\n",
    "# Training passo passo\n",
    "for epoch in range(epochs):\n",
    "    nn.partial_fit(X_dev, y_dev)\n",
    "\n",
    "    # Predizioni su training e test set\n",
    "    y_dev_pred = nn.predict(X_dev)\n",
    "    y_test_pred = nn.predict(X_test)\n",
    "\n",
    "    # Calcoliamo le metriche\n",
    "    train_loss = mean_squared_error(y_dev, y_dev_pred)  # MSE su training\n",
    "    test_loss = mean_squared_error(y_test, y_test_pred)  # MSE su test\n",
    "\n",
    "    # Accuracy: arrotondamento delle predizioni per la classificazione\n",
    "    train_acc = accuracy_score(y_dev, y_dev_pred.round())\n",
    "    test_acc = accuracy_score(y_test, y_test_pred.round())\n",
    "\n",
    "    # Memorizziamo i valori di loss e accuracy\n",
    "    train_loss_curve.append(train_loss)\n",
    "    test_loss_curve.append(test_loss)\n",
    "    train_accuracy_curve.append(train_acc)\n",
    "    test_accuracy_curve.append(test_acc)\n",
    "\n",
    "    # Stampa ogni 50 epoche\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoca {epoch + 1}/{epochs}, Train MSE: {train_loss:.5f}, Train Acc: {train_acc:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot delle metriche\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Primo grafico: Loss Curve\n",
    "ax1.set_xlabel(\"Epoche\")\n",
    "ax1.set_ylabel(\"MSE\")\n",
    "ax1.plot(train_loss_curve, label=\"Train Loss\", color=\"blue\", linestyle=\"solid\")\n",
    "ax1.plot(test_loss_curve, label=\"Test Loss\", color=\"red\", linestyle=\"solid\")\n",
    "ax1.legend(loc=\"upper right\")\n",
    "ax1.set_title(\"MONK-1 Loss\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# Secondo grafico: Accuracy Curve\n",
    "ax2.set_xlabel(\"Epoche\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.plot(train_accuracy_curve, label=\"Train Accuracy\", color=\"blue\", linestyle=\"solid\")\n",
    "ax2.plot(test_accuracy_curve, label=\"Test Accuracy\", color=\"red\", linestyle=\"solid\")\n",
    "ax2.legend(loc=\"lower right\")\n",
    "ax2.set_title(\"MONK-1 Accuracy\")\n",
    "ax2.grid(True)\n",
    "\n",
    "# Migliora la disposizione dei grafici\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(F'Training Accuracy: {accuracy_score(y_dev, nn.predict(X_dev).round()):.4f}')\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, nn.predict(X_test).round()):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 5 run diverse corrispondenti a 5 random_state differenti (MODEL SELECTION)\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "# Per salvare i risultati delle 5 inizializzazioni differenti\n",
    "all_train_mse = []\n",
    "all_val_mse = []\n",
    "all_train_acc = []\n",
    "all_val_acc = []\n",
    "\n",
    "# Seed differenti\n",
    "random_states = [7, 13, 26, 39, 47]\n",
    "\n",
    "for rs in random_states:\n",
    "    print(f\"Training con random_state={rs}...\")\n",
    "\n",
    "    # Inizializzazione modello con i miglior iperparametri trovati\n",
    "    nn = MLPRegressor(\n",
    "        hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "        activation=best_params['activation'],\n",
    "        solver=best_params['solver'],\n",
    "        learning_rate_init=best_params['learning_rate_init'],\n",
    "        batch_size=batch_size,  # fullbatch (99)\n",
    "        **({'learning_rate': best_params['learning_rate']} if best_params['solver'] == 'sgd' else {}), # learning_rate solo se il solver è 'sgd'\n",
    "        **({'momentum': best_params['momentum']} if best_params['solver'] == 'sgd' else {}), # momentum solo se il solver è 'sgd'\n",
    "        max_iter=1, # nota bene\n",
    "        warm_start=True, # nota bene\n",
    "        shuffle=True,\n",
    "        random_state=rs,\n",
    "        verbose=False\n",
    "      )\n",
    "\n",
    "    train_mse_list = []\n",
    "    val_mse_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    for epoch in range(epochs):  \n",
    "        nn.partial_fit(X_train, y_train)\n",
    "        \n",
    "        # Predizioni su training e test set\n",
    "        y_train_pred = nn.predict(X_train)\n",
    "        y_val_pred = nn.predict(X_val)  \n",
    "\n",
    "        # Calcolo metriche     \n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "        # Accuracy: arrotondamento delle predizioni per la classificazione\n",
    "        train_acc = accuracy_score(y_train, y_train_pred.round())\n",
    "        val_acc = accuracy_score(y_val, y_val_pred.round()) \n",
    "\n",
    "        # Memorizziamo i valori di loss e accuracy\n",
    "        train_mse_list.append(train_mse)    \n",
    "        val_mse_list.append(val_mse)\n",
    "        train_acc_list.append(train_acc)\n",
    "        val_acc_list.append(val_acc)\n",
    "\n",
    "\n",
    "    # Salviamo i risultati di questa inizializzazione (per la media)\n",
    "    all_train_mse.append(train_mse_list)\n",
    "    all_val_mse.append(val_mse_list)\n",
    "    all_train_acc.append(train_acc_list)\n",
    "    all_val_acc.append(val_acc_list)\n",
    "\n",
    "    # Creazione della figura con due subplot\n",
    "    fig, ax1 = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot MSE\n",
    "    ax1[0].plot(train_mse_list, label=\"Train MSE\", color=\"blue\")\n",
    "    ax1[0].plot(val_mse_list, label=\"Validation MSE\", color=\"green\")\n",
    "    ax1[0].set_xlabel(\"Epochs\")\n",
    "    ax1[0].set_ylabel(\"MSE\")\n",
    "    ax1[0].set_title(f\"MSE MONK-1 (random_state={rs})\")\n",
    "    ax1[0].legend()\n",
    "    ax1[0].grid(True)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    ax1[1].plot(train_acc_list, label=\"Train Accuracy\", color=\"blue\")\n",
    "    ax1[1].plot(val_acc_list, label=\"Validation Accuracy\", color=\"green\")\n",
    "    ax1[1].set_xlabel(\"Epochs\")\n",
    "    ax1[1].set_ylabel(\"Accuracy\")\n",
    "    ax1[1].set_title(f\"Accuracy MONK-1 (random_state={rs})\")\n",
    "    ax1[1].legend()\n",
    "    ax1[1].grid(True)\n",
    "\n",
    "    # Mostra la figura completa con i due subplot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(F'Training Accuracy: {accuracy_score(y_train, nn.predict(X_train).round()):.4f}')\n",
    "    print(f'Validation Accuracy: {accuracy_score(y_val, nn.predict(X_val).round()):.4f}')\n",
    "    print(f'Best params: {best_params}')\n",
    "\n",
    "# Calcolo medie\n",
    "mean_train_mse = np.mean(all_train_mse, axis=0)\n",
    "mean_val_mse = np.mean(all_val_mse, axis=0)\n",
    "mean_train_acc = np.mean(all_train_acc, axis=0)\n",
    "mean_val_acc = np.mean(all_val_acc, axis=0)\n",
    "\n",
    "# Plot medie\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(mean_train_mse, label=\"Mean Train MSE\", color=\"blue\")\n",
    "plt.plot(mean_val_mse, label=\"Mean Validation MSE\", color=\"green\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Media MSE su 5 random_state MONK-1\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(mean_train_acc, label=\"Mean Train Accuracy\", color=\"blue\")\n",
    "plt.plot(mean_val_acc, label=\"Mean Validation Accuracy\", color=\"green\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Media Accuracy su 5 random_state MONK-1\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model MONK-1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "best_params = {'activation': 'tanh', \n",
    "               'hidden_layer_sizes': (3,), \n",
    "               'learning_rate_init': 0.02, \n",
    "               'solver': 'adam'}\n",
    "\n",
    "# Inizializzazione modello con i miglior iperparametri trovati\n",
    "nn = MLPRegressor(\n",
    "        hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "        activation=best_params['activation'],\n",
    "        solver=best_params['solver'],\n",
    "        learning_rate_init=best_params['learning_rate_init'],\n",
    "        batch_size=batch_size,  # fullbatch (99)\n",
    "        max_iter=1, # nota bene\n",
    "        warm_start=True, # nota bene\n",
    "        shuffle=True,\n",
    "        random_state=7, # MIGLIOR MODELLO TRAMITE MODEL SELECTION\n",
    "        verbose=False\n",
    "      )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TRAINING MODELLO\n",
    "\n",
    "# Numero di epoche\n",
    "epochs = 250  \n",
    "\n",
    "# Liste per tenere traccia delle metriche epoca per epoca\n",
    "train_loss_curve = []\n",
    "test_loss_curve = []\n",
    "train_accuracy_curve = []\n",
    "test_accuracy_curve = []\n",
    "\n",
    "# Training passo passo\n",
    "for epoch in range(epochs):\n",
    "    nn.partial_fit(X_dev, y_dev)\n",
    "\n",
    "    # Predizioni su training e test set\n",
    "    y_dev_pred = nn.predict(X_dev)\n",
    "    y_test_pred = nn.predict(X_test)\n",
    "\n",
    "    # Calcoliamo le metriche\n",
    "    train_loss = mean_squared_error(y_dev, y_dev_pred)  # MSE su training\n",
    "    test_loss = mean_squared_error(y_test, y_test_pred)  # MSE su test\n",
    "\n",
    "    # Accuracy: arrotondamento delle predizioni per la classificazione\n",
    "    train_acc = accuracy_score(y_dev, y_dev_pred.round())\n",
    "    test_acc = accuracy_score(y_test, y_test_pred.round())\n",
    "\n",
    "    # Memorizziamo i valori di loss e accuracy\n",
    "    train_loss_curve.append(train_loss)\n",
    "    test_loss_curve.append(test_loss)\n",
    "    train_accuracy_curve.append(train_acc)\n",
    "    test_accuracy_curve.append(test_acc)\n",
    "\n",
    "    # Stampa ogni 50 epoche\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoca {epoch + 1}/{epochs}, Train MSE: {train_loss:.5f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, Test MSE: {test_loss:.5f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot delle metriche\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Primo grafico: Loss Curve\n",
    "ax1.set_xlabel(\"Epoche\")\n",
    "ax1.set_ylabel(\"MSE\")\n",
    "ax1.plot(train_loss_curve, label=\"Train Loss\", color=\"blue\", linestyle=\"solid\")\n",
    "ax1.plot(test_loss_curve, label=\"Test Loss\", color=\"red\", linestyle=\"solid\")\n",
    "ax1.legend(loc=\"upper right\")\n",
    "ax1.set_title(\"MONK-1 Loss\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# Secondo grafico: Accuracy Curve\n",
    "ax2.set_xlabel(\"Epoche\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.plot(train_accuracy_curve, label=\"Train Accuracy\", color=\"blue\", linestyle=\"solid\")\n",
    "ax2.plot(test_accuracy_curve, label=\"Test Accuracy\", color=\"red\", linestyle=\"solid\")\n",
    "ax2.legend(loc=\"lower right\")\n",
    "ax2.set_title(\"MONK-1 Accuracy\")\n",
    "ax2.grid(True)\n",
    "\n",
    "# Migliora la disposizione dei grafici\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"Numero di epoche salvate: {len(train_loss_curve)}\")\n",
    "print(f\"Numero di epoche previste: {epochs}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "print(\"\\nMiglior modello trovato per MONK-1 (random state 7 - 250 epochs):\")\n",
    "print(f\"Parametri: {best_params}\")\n",
    "\n",
    "print(F'Training Accuracy: {accuracy_score(y_dev, nn.predict(X_dev).round()):.4f}')\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, nn.predict(X_test).round()):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monk 2\n",
    "MONK-2: EXACTLY TWO of {a1 = 1, a2 = 1, a3 = 1, a4 = 1, a5 = 1, a6 = 1} --> Class label 1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# lettura dataset monk 2\n",
    "monk_train_2 = load_monk_data_one_hot_enc(\"../data/monk+s+problems/monks-2.train\")\n",
    "monk_test_2 = load_monk_data_one_hot_enc(\"../data/monk+s+problems/monks-2.test\")\n",
    "\n",
    "monk_train_2.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(monk_train_2.shape)\n",
    "print(monk_test_2.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# separazione feature e target\n",
    "X_dev, y_dev = monk_train_2.iloc[:, 1:], monk_train_2.iloc[:, 0]\n",
    "X_test, y_test = monk_test_2.iloc[:, 1:], monk_test_2.iloc[:, 0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(X_dev.shape)\n",
    "print(y_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Splitting del dev set in training e validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "batch_size = X_train.shape[0]\n",
    "print(batch_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Iperparametri da testare \n",
    "iperparameters = [ \n",
    "    {\n",
    "    'hidden_layer_sizes': [(2,), (3,), (4,)],  \n",
    "    'activation': ['tanh', 'relu', 'logistic'],  \n",
    "    'solver': ['adam'], \n",
    "    'learning_rate_init': [0.01, 0.02, 0.05, 0.001, 0.002],\n",
    "    },\n",
    "    {\n",
    "    'hidden_layer_sizes': [(2,), (3,), (4,)],  \n",
    "    'activation': ['tanh', 'relu', 'logistic'],  \n",
    "    'solver': ['sgd'],  \n",
    "    'learning_rate_init': [0.01, 0.02, 0.05, 0.001, 0.002],\n",
    "    'learning_rate': ['constant', 'adaptive'],  # solo per SGD\n",
    "    'momentum': [0.9, 0.95]  # solo per SGD\n",
    "    }\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_train_acc = float('-inf')\n",
    "best_val_acc = float('-inf')\n",
    "best_train_mse = float('inf')\n",
    "best_val_mse = float('inf')\n",
    "best_params = None\n",
    "\n",
    "# Itera su tutte le combinazioni di iperparametri\n",
    "for params in ParameterGrid(iperparameters):\n",
    "    # Definizione del modello con gli iperparametri scelti\n",
    "    nn = MLPRegressor(\n",
    "        hidden_layer_sizes=params['hidden_layer_sizes'],\n",
    "        activation=params['activation'],\n",
    "        solver=params['solver'],\n",
    "        learning_rate_init=params['learning_rate_init'],\n",
    "        batch_size=batch_size,  # fullbatch (135)\n",
    "        **({'learning_rate': params['learning_rate']} if params['solver'] == 'sgd' else {}),  # learning_rate solo se il solver è 'sgd'\n",
    "        **({'momentum': params['momentum']} if params['solver'] == 'sgd' else {}),  # momentum solo se il solver è 'sgd'\n",
    "        max_iter=500,\n",
    "        shuffle=True,\n",
    "        random_state=7,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Addestramento del modello \n",
    "    nn.fit(X_train, y_train)\n",
    "\n",
    "    # Predizioni sui dati\n",
    "    y_pred_train = nn.predict(X_train)\n",
    "    y_pred_val = nn.predict(X_val)\n",
    "\n",
    "    # Calcolo MSE\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    val_mse = mean_squared_error(y_val, y_pred_val)\n",
    "\n",
    "    # Convertsione delle predizioni in classi discrete\n",
    "    train_acc = accuracy_score(y_train.round(), y_pred_train.round())\n",
    "    val_acc = accuracy_score(y_val.round(), y_pred_val.round())\n",
    "\n",
    "\n",
    "    # Verifica se abbiamo trovato un modello migliore\n",
    "    if val_acc > best_val_acc:  \n",
    "        best_train_mse = train_mse\n",
    "        best_val_mse = val_mse\n",
    "        best_train_acc = train_acc\n",
    "        best_val_acc = val_acc\n",
    "        best_model = nn\n",
    "        best_params = params"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "print(\"\\nMiglior modello trovato per MONK-2 (random state 7 - 500 epochs):\")\n",
    "print(f\"Parametri: {best_params}\")\n",
    "\n",
    "print(F'Training Accuracy: {accuracy_score(y_train, best_model.predict(X_train).round()):.4f}')\n",
    "print(f'Validation Accuracy: {accuracy_score(y_val, best_model.predict(X_val).round()):.4f}')\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, best_model.predict(X_test).round()):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Inizializzazione modello con i miglior iperparametri trovati\n",
    "nn = MLPRegressor(\n",
    "        hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "        activation=best_params['activation'],\n",
    "        solver=best_params['solver'],\n",
    "        learning_rate_init=best_params['learning_rate_init'],\n",
    "        batch_size=batch_size,  # fullbatch (135)\n",
    "        **({'learning_rate': best_params['learning_rate']} if best_params['solver'] == 'sgd' else {}), # learning_rate solo se il solver è 'sgd'\n",
    "        **({'momentum': best_params['momentum']} if best_params['solver'] == 'sgd' else {}), # momentum solo se il solver è 'sgd'\n",
    "        max_iter=1, # nota bene\n",
    "        warm_start=True, # nota bene\n",
    "        shuffle=True,\n",
    "        random_state=7,\n",
    "        verbose=False\n",
    "      )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TRAINING MODELLO\n",
    "\n",
    "# Numero di epoche\n",
    "epochs = 500  \n",
    "\n",
    "# Liste per tenere traccia delle metriche epoca per epoca\n",
    "train_loss_curve = []\n",
    "test_loss_curve = []\n",
    "train_accuracy_curve = []\n",
    "test_accuracy_curve = []\n",
    "\n",
    "# Training passo passo\n",
    "for epoch in range(epochs):\n",
    "    nn.partial_fit(X_dev, y_dev)\n",
    "\n",
    "    # Predizioni su training e test set\n",
    "    y_dev_pred = nn.predict(X_dev)\n",
    "    y_test_pred = nn.predict(X_test)\n",
    "\n",
    "    # Calcoliamo le metriche\n",
    "    train_loss = mean_squared_error(y_dev, y_dev_pred)  # MSE su training\n",
    "    test_loss = mean_squared_error(y_test, y_test_pred)  # MSE su test\n",
    "\n",
    "    # Accuracy: arrotondamento delle predizioni per la classificazione\n",
    "    train_acc = accuracy_score(y_dev, y_dev_pred.round())\n",
    "    test_acc = accuracy_score(y_test, y_test_pred.round())\n",
    "\n",
    "    # Memorizziamo i valori di loss e accuracy\n",
    "    train_loss_curve.append(train_loss)\n",
    "    test_loss_curve.append(test_loss)\n",
    "    train_accuracy_curve.append(train_acc)\n",
    "    test_accuracy_curve.append(test_acc)\n",
    "\n",
    "    # Stampa ogni 50 epoche\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoca {epoch + 1}/{epochs}, Train MSE: {train_loss:.5f}, Train Acc: {train_acc:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot delle metriche\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Primo grafico: Loss Curve\n",
    "ax1.set_xlabel(\"Epoche\")\n",
    "ax1.set_ylabel(\"MSE\")\n",
    "ax1.plot(train_loss_curve, label=\"Train Loss\", color=\"blue\", linestyle=\"solid\")\n",
    "ax1.plot(test_loss_curve, label=\"Test Loss\", color=\"red\", linestyle=\"solid\")\n",
    "ax1.legend(loc=\"upper right\")\n",
    "ax1.set_title(\"MONK-2 Loss\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# Secondo grafico: Accuracy Curve\n",
    "ax2.set_xlabel(\"Epoche\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.plot(train_accuracy_curve, label=\"Train Accuracy\", color=\"blue\", linestyle=\"solid\")\n",
    "ax2.plot(test_accuracy_curve, label=\"Test Accuracy\", color=\"red\", linestyle=\"solid\")\n",
    "ax2.legend(loc=\"lower right\")\n",
    "ax2.set_title(\"MONK-2 Accuracy\")\n",
    "ax2.grid(True)\n",
    "\n",
    "# Migliora la disposizione dei grafici\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(F'Training Accuracy: {accuracy_score(y_dev, nn.predict(X_dev).round()):.4f}')\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, nn.predict(X_test).round()):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 5 run diverse corrispondenti a 5 random_state differenti (MODEL SELECTION)\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "# Per salvare i risultati delle 5 inizializzazioni differenti\n",
    "all_train_mse = []\n",
    "all_val_mse = []\n",
    "all_train_acc = []\n",
    "all_val_acc = []\n",
    "\n",
    "# Seed differenti\n",
    "random_states = [7, 18, 28, 31, 42]\n",
    "\n",
    "for rs in random_states:\n",
    "    print(f\"Training con random_state={rs}...\")\n",
    "\n",
    "    # Inizializzazione modello con i miglior iperparametri trovati\n",
    "    nn = MLPRegressor(\n",
    "        hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "        activation=best_params['activation'],\n",
    "        solver=best_params['solver'],\n",
    "        learning_rate_init=best_params['learning_rate_init'],\n",
    "        batch_size=batch_size,  # fullbatch (135)\n",
    "        **({'learning_rate': best_params['learning_rate']} if best_params['solver'] == 'sgd' else {}), # learning_rate solo se il solver è 'sgd'\n",
    "        **({'momentum': best_params['momentum']} if best_params['solver'] == 'sgd' else {}), # momentum solo se il solver è 'sgd'\n",
    "        max_iter=1, # nota bene\n",
    "        warm_start=True, # nota bene\n",
    "        shuffle=True,\n",
    "        random_state=rs,\n",
    "        verbose=False\n",
    "      )\n",
    "\n",
    "    train_mse_list = []\n",
    "    val_mse_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    for epoch in range(epochs):  \n",
    "        nn.partial_fit(X_train, y_train)\n",
    "        \n",
    "        # Predizioni su training e test set\n",
    "        y_train_pred = nn.predict(X_train)\n",
    "        y_val_pred = nn.predict(X_val)  \n",
    "\n",
    "        # Calcolo metriche     \n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "        # Accuracy: arrotondamento delle predizioni per la classificazione\n",
    "        train_acc = accuracy_score(y_train, y_train_pred.round())\n",
    "        val_acc = accuracy_score(y_val, y_val_pred.round()) \n",
    "\n",
    "        # Memorizziamo i valori di loss e accuracy\n",
    "        train_mse_list.append(train_mse)    \n",
    "        val_mse_list.append(val_mse)\n",
    "        train_acc_list.append(train_acc)\n",
    "        val_acc_list.append(val_acc)\n",
    "\n",
    "\n",
    "    # Salviamo i risultati di questa inizializzazione (per la media)\n",
    "    all_train_mse.append(train_mse_list)\n",
    "    all_val_mse.append(val_mse_list)\n",
    "    all_train_acc.append(train_acc_list)\n",
    "    all_val_acc.append(val_acc_list)\n",
    "\n",
    "    # Creazione della figura con due subplot\n",
    "    fig, ax1 = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot MSE\n",
    "    ax1[0].plot(train_mse_list, label=\"Train MSE\", color=\"blue\")\n",
    "    ax1[0].plot(val_mse_list, label=\"Validation MSE\", color=\"green\")\n",
    "    ax1[0].set_xlabel(\"Epochs\")\n",
    "    ax1[0].set_ylabel(\"MSE\")\n",
    "    ax1[0].set_title(f\"MSE MONK-2 (random_state={rs})\")\n",
    "    ax1[0].legend()\n",
    "    ax1[0].grid(True)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    ax1[1].plot(train_acc_list, label=\"Train Accuracy\", color=\"blue\")\n",
    "    ax1[1].plot(val_acc_list, label=\"Validation Accuracy\", color=\"green\")\n",
    "    ax1[1].set_xlabel(\"Epochs\")\n",
    "    ax1[1].set_ylabel(\"Accuracy\")\n",
    "    ax1[1].set_title(f\"Accuracy MONK-2 (random_state={rs})\")\n",
    "    ax1[1].legend()\n",
    "    ax1[1].grid(True)\n",
    "\n",
    "    # Mostra la figura completa con i due subplot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(F'Training Accuracy: {accuracy_score(y_train, nn.predict(X_train).round()):.4f}')\n",
    "    print(f'Validation Accuracy: {accuracy_score(y_val, nn.predict(X_val).round()):.4f}')\n",
    "    print(f'Best params: {best_params}')\n",
    "\n",
    "# Calcolo medie\n",
    "mean_train_mse = np.mean(all_train_mse, axis=0)\n",
    "mean_val_mse = np.mean(all_val_mse, axis=0)\n",
    "mean_train_acc = np.mean(all_train_acc, axis=0)\n",
    "mean_val_acc = np.mean(all_val_acc, axis=0)\n",
    "\n",
    "# Plot medie\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(mean_train_mse, label=\"Mean Train MSE\", color=\"blue\")\n",
    "plt.plot(mean_val_mse, label=\"Mean Validation MSE\", color=\"green\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Media MSE su 5 random_state MONK-2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(mean_train_acc, label=\"Mean Train Accuracy\", color=\"blue\")\n",
    "plt.plot(mean_val_acc, label=\"Mean Validation Accuracy\", color=\"green\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Media Accuracy su 5 random_state MONK-2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model MONK-2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "best_params = {'activation': 'tanh', \n",
    "               'hidden_layer_sizes': (2,), \n",
    "               'learning_rate_init': 0.01, \n",
    "               'solver': 'adam'}\n",
    "\n",
    "# Inizializzazione modello con i miglior iperparametri trovati\n",
    "nn = MLPRegressor(\n",
    "        hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "        activation=best_params['activation'],\n",
    "        solver=best_params['solver'],\n",
    "        learning_rate_init=best_params['learning_rate_init'],\n",
    "        batch_size=batch_size,  # fullbatch (135)\n",
    "        max_iter=1, # nota bene\n",
    "        warm_start=True, # nota bene\n",
    "        shuffle=True,\n",
    "        random_state=28, # MIGLIOR MODELLO TRAMITE MODEL SELECTION\n",
    "        verbose=False\n",
    "      )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TRAINING MODELLO\n",
    "\n",
    "# Numero di epoche\n",
    "epochs = 300  \n",
    "\n",
    "# Liste per tenere traccia delle metriche epoca per epoca\n",
    "train_loss_curve = []\n",
    "test_loss_curve = []\n",
    "train_accuracy_curve = []\n",
    "test_accuracy_curve = []\n",
    "\n",
    "# Training passo passo\n",
    "for epoch in range(epochs):\n",
    "    nn.partial_fit(X_dev, y_dev)\n",
    "\n",
    "    # Predizioni su training e test set\n",
    "    y_dev_pred = nn.predict(X_dev)\n",
    "    y_test_pred = nn.predict(X_test)\n",
    "\n",
    "    # Calcoliamo le metriche\n",
    "    train_loss = mean_squared_error(y_dev, y_dev_pred)  # MSE su training\n",
    "    test_loss = mean_squared_error(y_test, y_test_pred)  # MSE su test\n",
    "\n",
    "    # Accuracy: arrotondamento delle predizioni per la classificazione\n",
    "    train_acc = accuracy_score(y_dev, y_dev_pred.round())\n",
    "    test_acc = accuracy_score(y_test, y_test_pred.round())\n",
    "\n",
    "    # Memorizziamo i valori di loss e accuracy\n",
    "    train_loss_curve.append(train_loss)\n",
    "    test_loss_curve.append(test_loss)\n",
    "    train_accuracy_curve.append(train_acc)\n",
    "    test_accuracy_curve.append(test_acc)\n",
    "\n",
    "    # Stampa ogni 50 epoche\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoca {epoch + 1}/{epochs}, Train MSE: {train_loss:.5f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, Test MSE: {test_loss:.5f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot delle metriche\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Primo grafico: Loss Curve\n",
    "ax1.set_xlabel(\"Epoche\")\n",
    "ax1.set_ylabel(\"MSE\")\n",
    "ax1.plot(train_loss_curve, label=\"Train Loss\", color=\"blue\", linestyle=\"solid\")\n",
    "ax1.plot(test_loss_curve, label=\"Test Loss\", color=\"red\", linestyle=\"solid\")\n",
    "ax1.legend(loc=\"upper right\")\n",
    "ax1.set_title(\"MONK-2 Loss\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# Secondo grafico: Accuracy Curve\n",
    "ax2.set_xlabel(\"Epoche\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.plot(train_accuracy_curve, label=\"Train Accuracy\", color=\"blue\", linestyle=\"solid\")\n",
    "ax2.plot(test_accuracy_curve, label=\"Test Accuracy\", color=\"red\", linestyle=\"solid\")\n",
    "ax2.legend(loc=\"lower right\")\n",
    "ax2.set_title(\"MONK-2 Accuracy\")\n",
    "ax2.grid(True)\n",
    "\n",
    "# Migliora la disposizione dei grafici\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "print(\"\\nMiglior modello trovato per MONK-2 (random state 28 - 300 epochs):\")\n",
    "print(f\"Parametri: {best_params}\")\n",
    "\n",
    "print(F'Training Accuracy: {accuracy_score(y_dev, nn.predict(X_dev).round()):.4f}')\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, nn.predict(X_test).round()):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monk 3\n",
    "MONK-3: (a5 = 3 and a4 = 1) or (a5 /= 4 and a2 /= 3)\n",
    "           (5% class noise added to the training set) --> Class label 1 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Senza regolarizzazione"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# lettura dataset monk 3\n",
    "monk_train_3 = load_monk_data_one_hot_enc(\"../data/monk+s+problems/monks-3.train\")\n",
    "monk_test_3 = load_monk_data_one_hot_enc(\"../data/monk+s+problems/monks-3.test\")\n",
    "\n",
    "# Stampiamo i primi 5 record\n",
    "monk_train_3.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(monk_train_3.shape)\n",
    "print(monk_test_3.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# separazione feature e target\n",
    "X_dev, y_dev = monk_train_3.iloc[:, 1:], monk_train_3.iloc[:, 0]\n",
    "X_test, y_test = monk_test_3.iloc[:, 1:], monk_test_3.iloc[:, 0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(X_dev.shape)\n",
    "print(y_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Splitting del dev set in training e validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "batch_size = X_train.shape[0]\n",
    "print(batch_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Iperparametri da testare \n",
    "iperparameters = [ \n",
    "    {\n",
    "    'hidden_layer_sizes': [(2,), (3,), (4,)],  \n",
    "    'activation': ['tanh', 'relu', 'logistic'],  \n",
    "    'solver': ['adam'], \n",
    "    'learning_rate_init': [0.01, 0.02, 0.05, 0.001, 0.002],\n",
    "    'alpha': [0] # senza regolarizzazione\n",
    "    },\n",
    "    {\n",
    "    'hidden_layer_sizes': [(2,), (3,), (4,)],  \n",
    "    'activation': ['tanh', 'relu', 'logistic'],  \n",
    "    'solver': ['sgd'],  \n",
    "    'learning_rate_init': [0.01, 0.02, 0.05, 0.001, 0.002],\n",
    "    'alpha': [0], # senza regolarizzazione\n",
    "    'learning_rate': ['constant', 'adaptive'],  # solo per SGD\n",
    "    'momentum': [0.9, 0.95]  # solo per SGD\n",
    "    }\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_train_acc = float('-inf')\n",
    "best_val_acc = float('-inf')\n",
    "best_train_mse = float('inf')\n",
    "best_val_mse = float('inf')\n",
    "best_params = None\n",
    "\n",
    "# Itera su tutte le combinazioni di iperparametri\n",
    "for params in ParameterGrid(iperparameters):\n",
    "    # Definizione del modello con gli iperparametri scelti\n",
    "    nn = MLPRegressor(\n",
    "        hidden_layer_sizes=params['hidden_layer_sizes'],\n",
    "        activation=params['activation'],\n",
    "        solver=params['solver'],\n",
    "        learning_rate_init=params['learning_rate_init'],\n",
    "        alpha=params['alpha'],\n",
    "        batch_size=batch_size,  # fullbatch (97)\n",
    "        **({'learning_rate': params['learning_rate']} if params['solver'] == 'sgd' else {}),  # learning_rate solo se il solver è 'sgd'\n",
    "        **({'momentum': params['momentum']} if params['solver'] == 'sgd' else {}),  # momentum solo se il solver è 'sgd'\n",
    "        max_iter=500,\n",
    "        shuffle=True,\n",
    "        random_state=7,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Addestramento del modello \n",
    "    nn.fit(X_train, y_train)\n",
    "\n",
    "    # Predizioni sui dati\n",
    "    y_pred_train = nn.predict(X_train)\n",
    "    y_pred_val = nn.predict(X_val)\n",
    "\n",
    "    # Calcolo MSE\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    val_mse = mean_squared_error(y_val, y_pred_val)\n",
    "\n",
    "    # Convertsione delle predizioni in classi discrete\n",
    "    train_acc = accuracy_score(y_train.round(), y_pred_train.round())\n",
    "    val_acc = accuracy_score(y_val.round(), y_pred_val.round())\n",
    "\n",
    "\n",
    "    # Verifica se abbiamo trovato un modello migliore\n",
    "    if val_acc > best_val_acc:  \n",
    "        best_train_mse = train_mse\n",
    "        best_val_mse = val_mse\n",
    "        best_train_acc = train_acc\n",
    "        best_val_acc = val_acc\n",
    "        best_model = nn\n",
    "        best_params = params"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "print(\"\\nMiglior modello trovato per MONK-3 (No Regularization) (random state 7 - 500 epochs):\")\n",
    "print(f\"Parametri: {best_params}\")\n",
    "\n",
    "print(F'Training Accuracy: {accuracy_score(y_train, best_model.predict(X_train).round()):.4f}')\n",
    "print(f'Validation Accuracy: {accuracy_score(y_val, best_model.predict(X_val).round()):.4f}')\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, best_model.predict(X_test).round()):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Inizializzazione modello con i miglior iperparametri trovati\n",
    "nn = MLPRegressor(\n",
    "        hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "        activation=best_params['activation'],\n",
    "        solver=best_params['solver'],\n",
    "        learning_rate_init=best_params['learning_rate_init'],\n",
    "        alpha=best_params['alpha'],\n",
    "        batch_size=batch_size,  # fullbatch (97)\n",
    "        **({'learning_rate': best_params['learning_rate']} if best_params['solver'] == 'sgd' else {}), # learning_rate solo se il solver è 'sgd'\n",
    "        **({'momentum': best_params['momentum']} if best_params['solver'] == 'sgd' else {}), # momentum solo se il solver è 'sgd'\n",
    "        max_iter=1, # nota bene\n",
    "        warm_start=True, # nota bene\n",
    "        shuffle=True,\n",
    "        random_state=7,\n",
    "        verbose=False\n",
    "      )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TRAINING MODELLO\n",
    "\n",
    "# Numero di epoche\n",
    "epochs = 500  \n",
    "\n",
    "# Liste per tenere traccia delle metriche epoca per epoca\n",
    "train_loss_curve = []\n",
    "test_loss_curve = []\n",
    "train_accuracy_curve = []\n",
    "test_accuracy_curve = []\n",
    "\n",
    "# Training passo passo\n",
    "for epoch in range(epochs):\n",
    "    nn.partial_fit(X_dev, y_dev)\n",
    "\n",
    "    # Predizioni su training e test set\n",
    "    y_dev_pred = nn.predict(X_dev)\n",
    "    y_test_pred = nn.predict(X_test)\n",
    "\n",
    "    # Calcoliamo le metriche\n",
    "    train_loss = mean_squared_error(y_dev, y_dev_pred)  # MSE su training\n",
    "    test_loss = mean_squared_error(y_test, y_test_pred)  # MSE su test\n",
    "\n",
    "    # Accuracy: arrotondamento delle predizioni per la classificazione\n",
    "    train_acc = accuracy_score(y_dev, y_dev_pred.round())\n",
    "    test_acc = accuracy_score(y_test, y_test_pred.round())\n",
    "\n",
    "    # Memorizziamo i valori di loss e accuracy\n",
    "    train_loss_curve.append(train_loss)\n",
    "    test_loss_curve.append(test_loss)\n",
    "    train_accuracy_curve.append(train_acc)\n",
    "    test_accuracy_curve.append(test_acc)\n",
    "\n",
    "    # Stampa ogni 50 epoche\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoca {epoch + 1}/{epochs}, Train MSE: {train_loss:.5f}, Train Acc: {train_acc:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot delle metriche\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Primo grafico: Loss Curve\n",
    "ax1.set_xlabel(\"Epoche\")\n",
    "ax1.set_ylabel(\"MSE\")\n",
    "ax1.plot(train_loss_curve, label=\"Train Loss\", color=\"blue\", linestyle=\"solid\")\n",
    "ax1.plot(test_loss_curve, label=\"Test Loss\", color=\"red\", linestyle=\"solid\")\n",
    "ax1.legend(loc=\"upper right\")\n",
    "ax1.set_title(\"MONK-3 Loss (No Regularization)\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# Secondo grafico: Accuracy Curve\n",
    "ax2.set_xlabel(\"Epoche\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.plot(train_accuracy_curve, label=\"Train Accuracy\", color=\"blue\", linestyle=\"solid\")\n",
    "ax2.plot(test_accuracy_curve, label=\"Test Accuracy\", color=\"red\", linestyle=\"solid\")\n",
    "ax2.legend(loc=\"lower right\")\n",
    "ax2.set_title(\"MONK-3 Accuracy (No Regularization)\")\n",
    "ax2.grid(True)\n",
    "\n",
    "# Migliora la disposizione dei grafici\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(F'Training Accuracy: {accuracy_score(y_dev, nn.predict(X_dev).round()):.4f}')\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, nn.predict(X_test).round()):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 5 run diverse corrispondenti a 5 random_state differenti (MODEL SELECTION)\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "# Per salvare i risultati delle 5 inizializzazioni differenti\n",
    "all_train_mse = []\n",
    "all_val_mse = []\n",
    "all_train_acc = []\n",
    "all_val_acc = []\n",
    "\n",
    "# Seed differenti\n",
    "random_states = [7, 18, 28, 31, 42]\n",
    "\n",
    "for rs in random_states:\n",
    "    print(f\"Training con random_state={rs}...\")\n",
    "\n",
    "    # Inizializzazione modello con i miglior iperparametri trovati\n",
    "    nn = MLPRegressor(\n",
    "        hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "        activation=best_params['activation'],\n",
    "        solver=best_params['solver'],\n",
    "        learning_rate_init=best_params['learning_rate_init'],\n",
    "        alpha=best_params['alpha'],\n",
    "        batch_size=batch_size,  # fullbatch (97)\n",
    "        **({'learning_rate': best_params['learning_rate']} if best_params['solver'] == 'sgd' else {}), # learning_rate solo se il solver è 'sgd'\n",
    "        **({'momentum': best_params['momentum']} if best_params['solver'] == 'sgd' else {}), # momentum solo se il solver è 'sgd'\n",
    "        max_iter=1, # nota bene\n",
    "        warm_start=True, # nota bene\n",
    "        shuffle=True,\n",
    "        random_state=rs,\n",
    "        verbose=False\n",
    "      )\n",
    "\n",
    "    train_mse_list = []\n",
    "    val_mse_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    for epoch in range(epochs):  \n",
    "        nn.partial_fit(X_train, y_train)\n",
    "        \n",
    "        # Predizioni su training e test set\n",
    "        y_train_pred = nn.predict(X_train)\n",
    "        y_val_pred = nn.predict(X_val)  \n",
    "\n",
    "        # Calcolo metriche     \n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "        # Accuracy: arrotondamento delle predizioni per la classificazione\n",
    "        train_acc = accuracy_score(y_train, y_train_pred.round())\n",
    "        val_acc = accuracy_score(y_val, y_val_pred.round()) \n",
    "\n",
    "        # Memorizziamo i valori di loss e accuracy\n",
    "        train_mse_list.append(train_mse)    \n",
    "        val_mse_list.append(val_mse)\n",
    "        train_acc_list.append(train_acc)\n",
    "        val_acc_list.append(val_acc)\n",
    "\n",
    "\n",
    "    # Salviamo i risultati di questa inizializzazione (per la media)\n",
    "    all_train_mse.append(train_mse_list)\n",
    "    all_val_mse.append(val_mse_list)\n",
    "    all_train_acc.append(train_acc_list)\n",
    "    all_val_acc.append(val_acc_list)\n",
    "\n",
    "    # Creazione della figura con due subplot\n",
    "    fig, ax1 = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot MSE\n",
    "    ax1[0].plot(train_mse_list, label=\"Train MSE\", color=\"blue\")\n",
    "    ax1[0].plot(val_mse_list, label=\"Validation MSE\", color=\"green\")\n",
    "    ax1[0].set_xlabel(\"Epochs\")\n",
    "    ax1[0].set_ylabel(\"MSE\")\n",
    "    ax1[0].set_title(f\"MSE MONK-3 (random_state={rs})\")\n",
    "    ax1[0].legend()\n",
    "    ax1[0].grid(True)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    ax1[1].plot(train_acc_list, label=\"Train Accuracy\", color=\"blue\")\n",
    "    ax1[1].plot(val_acc_list, label=\"Validation Accuracy\", color=\"green\")\n",
    "    ax1[1].set_xlabel(\"Epochs\")\n",
    "    ax1[1].set_ylabel(\"Accuracy\")\n",
    "    ax1[1].set_title(f\"Accuracy MONK-3 (random_state={rs})\")\n",
    "    ax1[1].legend()\n",
    "    ax1[1].grid(True)\n",
    "\n",
    "    # Mostra la figura completa con i due subplot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(F'Training Accuracy: {accuracy_score(y_train, nn.predict(X_train).round()):.4f}')\n",
    "    print(f'Validation Accuracy: {accuracy_score(y_val, nn.predict(X_val).round()):.4f}')\n",
    "    print(f'Best params: {best_params}')\n",
    "\n",
    "# Calcolo medie\n",
    "mean_train_mse = np.mean(all_train_mse, axis=0)\n",
    "mean_val_mse = np.mean(all_val_mse, axis=0)\n",
    "mean_train_acc = np.mean(all_train_acc, axis=0)\n",
    "mean_val_acc = np.mean(all_val_acc, axis=0)\n",
    "\n",
    "# Plot medie\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(mean_train_mse, label=\"Mean Train MSE\", color=\"blue\")\n",
    "plt.plot(mean_val_mse, label=\"Mean Validation MSE\", color=\"green\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Media MSE su 5 random_state MONK-3\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(mean_train_acc, label=\"Mean Train Accuracy\", color=\"blue\")\n",
    "plt.plot(mean_val_acc, label=\"Mean Validation Accuracy\", color=\"green\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Media Accuracy su 5 random_state MONK-3\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model MONK-3 (No Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "best_params = {'activation': 'relu',\n",
    " 'alpha': 0,\n",
    " 'hidden_layer_sizes': (4,),\n",
    " 'learning_rate_init': 0.001,\n",
    " 'solver': 'adam'}\n",
    "\n",
    "# Inizializzazione modello con i miglior iperparametri trovati\n",
    "nn = MLPRegressor(\n",
    "        hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "        activation=best_params['activation'],\n",
    "        solver=best_params['solver'],\n",
    "        learning_rate_init=best_params['learning_rate_init'],\n",
    "        alpha=best_params['alpha'],\n",
    "        batch_size=batch_size,  # fullbatch (97)\n",
    "        max_iter=1, # nota bene\n",
    "        warm_start=True, # nota bene\n",
    "        shuffle=True,\n",
    "        random_state=7, # MIGLIOR MODELLO TRAMITE MODEL SELECTION\n",
    "        verbose=False\n",
    "      )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TRAINING MODELLO\n",
    "\n",
    "# Numero di epoche\n",
    "epochs = 500  \n",
    "\n",
    "# Liste per tenere traccia delle metriche epoca per epoca\n",
    "train_loss_curve = []\n",
    "test_loss_curve = []\n",
    "train_accuracy_curve = []\n",
    "test_accuracy_curve = []\n",
    "\n",
    "# Training passo passo\n",
    "for epoch in range(epochs):\n",
    "    nn.partial_fit(X_dev, y_dev)\n",
    "\n",
    "    # Predizioni su training e test set\n",
    "    y_dev_pred = nn.predict(X_dev)\n",
    "    y_test_pred = nn.predict(X_test)\n",
    "\n",
    "    # Calcoliamo le metriche\n",
    "    train_loss = mean_squared_error(y_dev, y_dev_pred)  # MSE su training\n",
    "    test_loss = mean_squared_error(y_test, y_test_pred)  # MSE su test\n",
    "\n",
    "    # Accuracy: arrotondamento delle predizioni per la classificazione\n",
    "    train_acc = accuracy_score(y_dev, y_dev_pred.round())\n",
    "    test_acc = accuracy_score(y_test, y_test_pred.round())\n",
    "\n",
    "    # Memorizziamo i valori di loss e accuracy\n",
    "    train_loss_curve.append(train_loss)\n",
    "    test_loss_curve.append(test_loss)\n",
    "    train_accuracy_curve.append(train_acc)\n",
    "    test_accuracy_curve.append(test_acc)\n",
    "\n",
    "     # Stampa ogni 50 epoche\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoca {epoch + 1}/{epochs}, Train MSE: {train_loss:.5f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, Test MSE: {test_loss:.5f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot delle metriche\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Primo grafico: Loss Curve\n",
    "ax1.set_xlabel(\"Epoche\")\n",
    "ax1.set_ylabel(\"MSE\")\n",
    "ax1.plot(train_loss_curve, label=\"Train Loss\", color=\"blue\", linestyle=\"solid\")\n",
    "ax1.plot(test_loss_curve, label=\"Test Loss\", color=\"red\", linestyle=\"solid\")\n",
    "ax1.legend(loc=\"upper right\")\n",
    "ax1.set_title(\"MONK-3 Loss (No Regularization)\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# Secondo grafico: Accuracy Curve\n",
    "ax2.set_xlabel(\"Epoche\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.plot(train_accuracy_curve, label=\"Train Accuracy\", color=\"blue\", linestyle=\"solid\")\n",
    "ax2.plot(test_accuracy_curve, label=\"Test Accuracy\", color=\"red\", linestyle=\"solid\")\n",
    "ax2.legend(loc=\"lower right\")\n",
    "ax2.set_title(\"MONK-3 Accuracy (No Regularization)\")\n",
    "ax2.grid(True)\n",
    "\n",
    "# Migliora la disposizione dei grafici\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "print(\"\\nMiglior modello trovato per MONK-3 (random state 7 - 500 epochs):\")\n",
    "print(f\"Parametri: {best_params}\")\n",
    "\n",
    "print(F'Training Accuracy: {accuracy_score(y_dev, nn.predict(X_dev).round()):.4f}')\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, nn.predict(X_test).round()):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con regolarizzazione"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# lettura dataset monk 3\n",
    "monk_train_3 = load_monk_data_one_hot_enc(\"../data/monk+s+problems/monks-3.train\")\n",
    "monk_test_3 = load_monk_data_one_hot_enc(\"../data/monk+s+problems/monks-3.test\")\n",
    "\n",
    "# Stampiamo i primi 5 record\n",
    "monk_train_3.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(monk_train_3.shape)\n",
    "print(monk_test_3.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# separazione feature e target\n",
    "X_dev, y_dev = monk_train_3.iloc[:, 1:], monk_train_3.iloc[:, 0]\n",
    "X_test, y_test = monk_test_3.iloc[:, 1:], monk_test_3.iloc[:, 0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(X_dev.shape)\n",
    "print(y_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Splitting del dev set in training e validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "batch_size = X_train.shape[0]\n",
    "print(batch_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Iperparametri da testare \n",
    "iperparameters = [ \n",
    "    {\n",
    "    'hidden_layer_sizes': [(2,), (3,), (4,)],  \n",
    "    'activation': ['tanh', 'relu', 'logistic'],  \n",
    "    'solver': ['adam'], \n",
    "    'learning_rate_init': [0.01, 0.02, 0.05, 0.001, 0.002],\n",
    "    'alpha':[0.0001, 0.0005, 0.001, 0.005, 0.01] # parametro di regolarizzazione\n",
    "    },\n",
    "    {\n",
    "    'hidden_layer_sizes': [(2,), (3,), (4,)],  \n",
    "    'activation': ['tanh', 'relu', 'logistic'],  \n",
    "    'solver': ['sgd'],  \n",
    "    'learning_rate_init': [0.01, 0.02, 0.05, 0.001, 0.002],\n",
    "    'alpha':[0.0001, 0.0005, 0.001, 0.005, 0.01], # parametro di regolarizzazione\n",
    "    'learning_rate': ['constant', 'adaptive'],  # solo per SGD\n",
    "    'momentum': [0.9, 0.95]  # solo per SGD\n",
    "    }\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_train_acc = float('-inf')\n",
    "best_val_acc = float('-inf')\n",
    "best_train_mse = float('inf')\n",
    "best_val_mse = float('inf')\n",
    "best_params = None\n",
    "\n",
    "# Itera su tutte le combinazioni di iperparametri\n",
    "for params in ParameterGrid(iperparameters):\n",
    "    # Definizione del modello con gli iperparametri scelti\n",
    "    nn = MLPRegressor(\n",
    "        hidden_layer_sizes=params['hidden_layer_sizes'],\n",
    "        activation=params['activation'],\n",
    "        solver=params['solver'],\n",
    "        learning_rate_init=params['learning_rate_init'],\n",
    "        alpha=params['alpha'],\n",
    "        batch_size=batch_size,  # fullbatch (97)\n",
    "        **({'learning_rate': params['learning_rate']} if params['solver'] == 'sgd' else {}),  # learning_rate solo se il solver è 'sgd'\n",
    "        **({'momentum': params['momentum']} if params['solver'] == 'sgd' else {}),  # momentum solo se il solver è 'sgd'\n",
    "        max_iter=500,\n",
    "        shuffle=True,\n",
    "        random_state=13,\n",
    "        verbose=False,\n",
    "        # early_stopping=True,  # EARLY STOPPING\n",
    "        # validation_fraction=0.2,  # 20% del training set usato per validazione\n",
    "        # n_iter_no_change=10,  # Stop dopo 20 epoche senza miglioramenti\n",
    "    )\n",
    "    \n",
    "    # Addestramento del modello \n",
    "    nn.fit(X_train, y_train)\n",
    "\n",
    "    # Predizioni sui dati\n",
    "    y_pred_train = nn.predict(X_train)\n",
    "    y_pred_val = nn.predict(X_val)\n",
    "\n",
    "    # Calcolo MSE\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    val_mse = mean_squared_error(y_val, y_pred_val)\n",
    "\n",
    "    # Convertsione delle predizioni in classi discrete\n",
    "    train_acc = accuracy_score(y_train.round(), y_pred_train.round())\n",
    "    val_acc = accuracy_score(y_val.round(), y_pred_val.round())\n",
    "\n",
    "    # Verifica se abbiamo trovato un modello migliore\n",
    "    if val_acc > best_val_acc:  \n",
    "        best_train_mse = train_mse\n",
    "        best_val_mse = val_mse\n",
    "        best_train_acc = train_acc\n",
    "        best_val_acc = val_acc\n",
    "        best_model = nn\n",
    "        best_params = params"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "print(\"\\nMiglior modello trovato per MONK-3 (Regularized) (random state 13 - 500 epochs):\")\n",
    "print(f\"Parametri: {best_params}\")\n",
    "\n",
    "print(F'Training Accuracy: {accuracy_score(y_train, best_model.predict(X_train).round()):.4f}')\n",
    "print(f'Validation Accuracy: {accuracy_score(y_val, best_model.predict(X_val).round()):.4f}')\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, best_model.predict(X_test).round()):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Inizializzazione modello con i miglior iperparametri trovati\n",
    "nn = MLPRegressor(\n",
    "        hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "        activation=best_params['activation'],\n",
    "        solver=best_params['solver'],\n",
    "        learning_rate_init=best_params['learning_rate_init'],\n",
    "        alpha=best_params['alpha'],\n",
    "        batch_size=batch_size,  # fullbatch (97)\n",
    "        **({'learning_rate': best_params['learning_rate']} if best_params['solver'] == 'sgd' else {}), # learning_rate solo se il solver è 'sgd'\n",
    "        **({'momentum': best_params['momentum']} if best_params['solver'] == 'sgd' else {}), # momentum solo se il solver è 'sgd'\n",
    "        max_iter=1, # nota bene\n",
    "        warm_start=True, # nota bene\n",
    "        shuffle=True,\n",
    "        random_state=13,\n",
    "        verbose=False,\n",
    "        # early_stopping=True,  # EARLY STOPPING\n",
    "        # validation_fraction=0.2,  # 20% del training set usato per validazione\n",
    "        # n_iter_no_change=20,  # Stop dopo 20 epoche senza miglioramenti\n",
    "      )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TRAINING MODELLO\n",
    "\n",
    "# Numero di epoche\n",
    "epochs = 500 \n",
    "\n",
    "# Liste per tenere traccia delle metriche epoca per epoca\n",
    "train_loss_curve = []\n",
    "test_loss_curve = []\n",
    "train_accuracy_curve = []\n",
    "test_accuracy_curve = []\n",
    "\n",
    "# Training passo passo\n",
    "for epoch in range(epochs):\n",
    "    nn.partial_fit(X_dev, y_dev)\n",
    "\n",
    "    # Predizioni su training e test set\n",
    "    y_dev_pred = nn.predict(X_dev)\n",
    "    y_test_pred = nn.predict(X_test)\n",
    "\n",
    "    # Calcoliamo le metriche\n",
    "    train_loss = mean_squared_error(y_dev, y_dev_pred)  # MSE su training\n",
    "    test_loss = mean_squared_error(y_test, y_test_pred)  # MSE su test\n",
    "\n",
    "    # Accuracy: arrotondamento delle predizioni per la classificazione\n",
    "    train_acc = accuracy_score(y_dev, y_dev_pred.round())\n",
    "    test_acc = accuracy_score(y_test, y_test_pred.round())\n",
    "\n",
    "    # Memorizziamo i valori di loss e accuracy\n",
    "    train_loss_curve.append(train_loss)\n",
    "    test_loss_curve.append(test_loss)\n",
    "    train_accuracy_curve.append(train_acc)\n",
    "    test_accuracy_curve.append(test_acc)\n",
    "\n",
    "    # Stampa ogni 50 epoche\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoca {epoch + 1}/{epochs}, Train MSE: {train_loss:.5f}, Train Acc: {train_acc:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot delle metriche\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Primo grafico: Loss Curve\n",
    "ax1.set_xlabel(\"Epoche\")\n",
    "ax1.set_ylabel(\"MSE\")\n",
    "ax1.plot(train_loss_curve, label=\"Train Loss\", color=\"blue\", linestyle=\"solid\")\n",
    "ax1.plot(test_loss_curve, label=\"Test Loss\", color=\"red\", linestyle=\"solid\")\n",
    "ax1.legend(loc=\"upper right\")\n",
    "ax1.set_title(\"MONK-3 Loss (Regularization)\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# Secondo grafico: Accuracy Curve\n",
    "ax2.set_xlabel(\"Epoche\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.plot(train_accuracy_curve, label=\"Train Accuracy\", color=\"blue\", linestyle=\"solid\")\n",
    "ax2.plot(test_accuracy_curve, label=\"Test Accuracy\", color=\"red\", linestyle=\"solid\")\n",
    "ax2.legend(loc=\"lower right\")\n",
    "ax2.set_title(\"MONK-3 Accuracy (Regularization)\")\n",
    "ax2.grid(True)\n",
    "\n",
    "# Migliora la disposizione dei grafici\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(F'Training Accuracy: {accuracy_score(y_dev, nn.predict(X_dev).round()):.4f}')\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, nn.predict(X_test).round()):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 5 run diverse corrispondenti a 5 random_state differenti (MODEL SELECTION)\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "# Per salvare i risultati delle 5 inizializzazioni differenti\n",
    "all_train_mse = []\n",
    "all_val_mse = []\n",
    "all_train_acc = []\n",
    "all_val_acc = []\n",
    "\n",
    "# Seed differenti\n",
    "random_states = [7, 13, 27, 31, 42]\n",
    "\n",
    "for rs in random_states:\n",
    "    print(f\"Training con random_state={rs}...\")\n",
    "\n",
    "    # Inizializzazione modello con i miglior iperparametri trovati\n",
    "    nn = MLPRegressor(\n",
    "        hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "        activation=best_params['activation'],\n",
    "        solver=best_params['solver'],\n",
    "        learning_rate_init=best_params['learning_rate_init'],\n",
    "        alpha=best_params['alpha'],\n",
    "        batch_size=batch_size,  # fullbatch (97)\n",
    "        **({'learning_rate': best_params['learning_rate']} if best_params['solver'] == 'sgd' else {}), # learning_rate solo se il solver è 'sgd'\n",
    "        **({'momentum': best_params['momentum']} if best_params['solver'] == 'sgd' else {}), # momentum solo se il solver è 'sgd'\n",
    "        max_iter=1, # nota bene\n",
    "        warm_start=True, # nota bene\n",
    "        shuffle=True,\n",
    "        random_state=rs,\n",
    "        verbose=False\n",
    "      )\n",
    "\n",
    "    train_mse_list = []\n",
    "    val_mse_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    for epoch in range(epochs):  \n",
    "        nn.partial_fit(X_train, y_train)\n",
    "        \n",
    "        # Predizioni su training e test set\n",
    "        y_train_pred = nn.predict(X_train)\n",
    "        y_val_pred = nn.predict(X_val)  \n",
    "\n",
    "        # Calcolo metriche     \n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "        # Accuracy: arrotondamento delle predizioni per la classificazione\n",
    "        train_acc = accuracy_score(y_train, y_train_pred.round())\n",
    "        val_acc = accuracy_score(y_val, y_val_pred.round()) \n",
    "\n",
    "        # Memorizziamo i valori di loss e accuracy\n",
    "        train_mse_list.append(train_mse)    \n",
    "        val_mse_list.append(val_mse)\n",
    "        train_acc_list.append(train_acc)\n",
    "        val_acc_list.append(val_acc)\n",
    "\n",
    "\n",
    "    # Salviamo i risultati di questa inizializzazione (per la media)\n",
    "    all_train_mse.append(train_mse_list)\n",
    "    all_val_mse.append(val_mse_list)\n",
    "    all_train_acc.append(train_acc_list)\n",
    "    all_val_acc.append(val_acc_list)\n",
    "\n",
    "    # Creazione della figura con due subplot\n",
    "    fig, ax1 = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot MSE\n",
    "    ax1[0].plot(train_mse_list, label=\"Train MSE\", color=\"blue\")\n",
    "    ax1[0].plot(val_mse_list, label=\"Validation MSE\", color=\"green\")\n",
    "    ax1[0].set_xlabel(\"Epochs\")\n",
    "    ax1[0].set_ylabel(\"MSE\")\n",
    "    ax1[0].set_title(f\"MSE MONK-3 (random_state={rs})\")\n",
    "    ax1[0].legend()\n",
    "    ax1[0].grid(True)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    ax1[1].plot(train_acc_list, label=\"Train Accuracy\", color=\"blue\")\n",
    "    ax1[1].plot(val_acc_list, label=\"Validation Accuracy\", color=\"green\")\n",
    "    ax1[1].set_xlabel(\"Epochs\")\n",
    "    ax1[1].set_ylabel(\"Accuracy\")\n",
    "    ax1[1].set_title(f\"Accuracy MONK-3 (random_state={rs})\")\n",
    "    ax1[1].legend()\n",
    "    ax1[1].grid(True)\n",
    "\n",
    "    # Mostra la figura completa con i due subplot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(F'Training Accuracy: {accuracy_score(y_train, nn.predict(X_train).round()):.4f}')\n",
    "    print(f'Validation Accuracy: {accuracy_score(y_val, nn.predict(X_val).round()):.4f}')\n",
    "    print(f'Best params: {best_params}')\n",
    "\n",
    "# Calcolo medie\n",
    "mean_train_mse = np.mean(all_train_mse, axis=0)\n",
    "mean_val_mse = np.mean(all_val_mse, axis=0)\n",
    "mean_train_acc = np.mean(all_train_acc, axis=0)\n",
    "mean_val_acc = np.mean(all_val_acc, axis=0)\n",
    "\n",
    "# Plot medie\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(mean_train_mse, label=\"Mean Train MSE\", color=\"blue\")\n",
    "plt.plot(mean_val_mse, label=\"Mean Validation MSE\", color=\"green\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Media MSE su 5 random_state MONK-3\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(mean_train_acc, label=\"Mean Train Accuracy\", color=\"blue\")\n",
    "plt.plot(mean_val_acc, label=\"Mean Validation Accuracy\", color=\"green\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Media Accuracy su 5 random_state MONK-3\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model MONK-3 (Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "best_params = {'activation': 'tanh',\n",
    " 'alpha': 0.0001,\n",
    " 'hidden_layer_sizes': (2,),\n",
    " 'learning_rate_init': 0.01,\n",
    " 'solver': 'adam'}\n",
    "\n",
    "# Inizializzazione modello con i miglior iperparametri trovati\n",
    "nn = MLPRegressor(\n",
    "        hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "        activation=best_params['activation'],\n",
    "        solver=best_params['solver'],\n",
    "        learning_rate_init=best_params['learning_rate_init'],\n",
    "        alpha=best_params['alpha'],\n",
    "        batch_size=batch_size,  # fullbatch (97)\n",
    "        max_iter=1, # nota bene\n",
    "        warm_start=True, # nota bene\n",
    "        shuffle=True,\n",
    "        random_state=13, # MIGLIOR MODELLO TRAMITE MODEL SELECTION\n",
    "        verbose=False\n",
    "      )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TRAINING MODELLO\n",
    "\n",
    "# Numero di epoche\n",
    "epochs = 300 \n",
    "\n",
    "# Liste per tenere traccia delle metriche epoca per epoca\n",
    "train_loss_curve = []\n",
    "test_loss_curve = []\n",
    "train_accuracy_curve = []\n",
    "test_accuracy_curve = []\n",
    "\n",
    "# Training passo passo\n",
    "for epoch in range(epochs):\n",
    "    nn.partial_fit(X_dev, y_dev)\n",
    "\n",
    "    # Predizioni su training e test set\n",
    "    y_dev_pred = nn.predict(X_dev)\n",
    "    y_test_pred = nn.predict(X_test)\n",
    "\n",
    "    # Calcoliamo le metriche\n",
    "    train_loss = mean_squared_error(y_dev, y_dev_pred)  # MSE su training\n",
    "    test_loss = mean_squared_error(y_test, y_test_pred)  # MSE su test\n",
    "\n",
    "    # Accuracy: arrotondamento delle predizioni per la classificazione\n",
    "    train_acc = accuracy_score(y_dev, y_dev_pred.round())\n",
    "    test_acc = accuracy_score(y_test, y_test_pred.round())\n",
    "\n",
    "    # Memorizziamo i valori di loss e accuracy\n",
    "    train_loss_curve.append(train_loss)\n",
    "    test_loss_curve.append(test_loss)\n",
    "    train_accuracy_curve.append(train_acc)\n",
    "    test_accuracy_curve.append(test_acc)\n",
    "\n",
    "    # Stampa ogni 50 epoche\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoca {epoch + 1}/{epochs}, Train MSE: {train_loss:.5f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, Test MSE: {test_loss:.5f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot delle metriche\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Primo grafico: Loss Curve\n",
    "ax1.set_xlabel(\"Epoche\")\n",
    "ax1.set_ylabel(\"MSE\")\n",
    "ax1.plot(train_loss_curve, label=\"Train Loss\", color=\"blue\", linestyle=\"solid\")\n",
    "ax1.plot(test_loss_curve, label=\"Test Loss\", color=\"red\", linestyle=\"solid\")\n",
    "ax1.legend(loc=\"upper right\")\n",
    "ax1.set_title(\"MONK-3 Loss (Regularization)\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# Secondo grafico: Accuracy Curve\n",
    "ax2.set_xlabel(\"Epoche\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.plot(train_accuracy_curve, label=\"Train Accuracy\", color=\"blue\", linestyle=\"solid\")\n",
    "ax2.plot(test_accuracy_curve, label=\"Test Accuracy\", color=\"red\", linestyle=\"solid\")\n",
    "ax2.legend(loc=\"lower right\")\n",
    "ax2.set_title(\"MONK-3 Accuracy (Regularization)\")\n",
    "ax2.grid(True)\n",
    "\n",
    "# Migliora la disposizione dei grafici\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "print(\"\\nMiglior modello trovato per MONK-3 (random state 13 - 300 epochs):\")\n",
    "print(f\"Parametri: {best_params}\")\n",
    "\n",
    "print(F'Training Accuracy: {accuracy_score(y_dev, nn.predict(X_dev).round()):.4f}')\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, nn.predict(X_test).round()):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
